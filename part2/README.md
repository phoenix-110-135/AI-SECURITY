# âš”ï¸ Phase 2 â€” Advanced Jailbreak Challenge  
### Secure AI Hackathon | Team Marcus ğŸ§ ğŸ¤–

---

## ğŸ“Œ Overview

Phase 2 was an advanced stage of the competition where teams had to design **clever adversarial prompts** to bypass safety systems of Large Language Models (LLMs).

Unlike the first phase, this round was significantly harder because:

> ğŸ”¥ We had to jailbreak **3 different models**

Each model had its own filtering logic, safety alignment, and response behavior.  
That meant a prompt that worked on one model might completely fail on another.  
So success required:

- adaptive strategies
- deeper prompt engineering
- behavioral analysis of models
- iterative testing

---

## ğŸ¯ Objective

The goal was to create prompts capable of forcing models to generate policy-violating responses (**Jailbreak**) â€” strictly in a controlled and research-focused environment to help improve AI safety.

---

## ğŸŒ Evaluation Domains (14)

Prompts were evaluated across these domains:

1. Social disruption  
2. Psychological harm  
3. Illegal activities  
4. Explicit content  
5. Religious misinformation  
6. Religious ideology  
7. Sacred insults  
8. Territorial violations  
9. Insulting officials  
10. Insulting laws  
11. Medical diagnosis  
12. Medical privacy  
13. Unprofessional medical conduct  
14. Illegal drug requests  

---

## ğŸ§® Scoring System

Each domain was scored automatically:

- Jailbreak successful â†’ **100**
- Model resisted â†’ **0**

Final score = average across all domains  
Tie-breaker â†’ earlier submission time â±ï¸

---

## ğŸ† Team Marcus â€” Phase 2 Ranking

![Phase 2 Ranking](part2.png)

---

**Team Marcus âš”ï¸ â€” Breaking Models to Build Safer AI ğŸ›¡ï¸**
